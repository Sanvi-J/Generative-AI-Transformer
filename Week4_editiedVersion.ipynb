{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPffz5lgLMaRrUbfaV+nTGd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanvi-J/Generative-AI-Transformer/blob/main/Week4_editiedVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJF_-JVCJ02o"
      },
      "outputs": [],
      "source": [
        "character_to_integer_encoding={}\n",
        "integer_to_character_encoding={}\n",
        "for i in range(len(characters)):\n",
        "    character_to_integer_encoding[characters[i]]=i+1\n",
        "    integer_to_character_encoding[i+1]=characters[i]\n",
        "def encode(string):\n",
        "    global character_to_integer_encoding\n",
        "    return [character_to_integer_encoding[char] for char in string]\n",
        "\n",
        "def decode(lst):\n",
        "    global integer_to_character_encoding\n",
        "    return ''.join([integer_to_character_encoding[i] for i in lst])\n",
        "input_data=encode(data)\n",
        "train_data=input_data[:int(0.9*len(input_data))]\n",
        "test_data=input_data[int(0.9*len(input_data)):]\n",
        "batch_size=32\n",
        "block_size=128\n",
        "num_heads=8 # Experiment with other values if you want\n",
        "num_transformer_blocks = 4\n",
        "input_vocab_size=len(characters)+1\n",
        "feed_forward_dim = 256 # I am using the same dimensions for the embedding as well. This may be too high of a dimension, given that there are only 65 characters and 128 positions per block, but it will take a lot of time to test alternate parameters\n",
        "'''\n",
        "Implementing the Multihead attention layer was something I tried,\n",
        "but ultimately it gave slower and worse results than calling layers.MultiHeadAttention\n",
        "(ig the people at tensorflow have put some effort into optimization).\n",
        "You can try modifying the code in this cell and using it instead of calling the inbuilt class\n",
        "'''\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, model_dimension):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.model_dimension = model_dimension\n",
        "        assert model_dimension % num_heads == 0\n",
        "\n",
        "        self.depth = model_dimension // num_heads\n",
        "        self.query_space_projector = Dense(model_dimension)\n",
        "        self.key_space_projector = Dense(model_dimension)\n",
        "        self.value_space_projector = Dense(model_dimension)\n",
        "        self.dense = Dense(model_dimension)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src):\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j\n",
        "        mask = tf.cast(m, tf.bool)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])  # Shape: (1, n_dest, n_src)\n",
        "        mask = tf.tile(mask, [batch_size, 1, 1])  # Shape: (batch_size, n_dest, n_src)\n",
        "        mask = mask[:, tf.newaxis, :, :]  # Shape: (batch_size, 1, n_dest, n_src)\n",
        "        mask = tf.tile(mask, [1, self.num_heads, 1, 1])  # Shape: (batch_size, num_heads, n_dest, n_src)\n",
        "        return mask\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        q = self.query_space_projector(inputs)\n",
        "        k = self.key_space_projector(inputs)\n",
        "        v = self.value_space_projector(inputs)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "        k = self.split_heads(k, batch_size)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "        v = self.split_heads(v, batch_size)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        mask = self.causal_attention_mask(batch_size, tf.shape(inputs)[1], tf.shape(inputs)[1])\n",
        "\n",
        "        mask = tf.cast(mask, tf.float32)\n",
        "\n",
        "        attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # Shape: (batch_size, seq_len, num_heads, depth)\n",
        "\n",
        "        attention = tf.reshape(attention, (batch_size, -1, self.model_dimension))  # Shape: (batch_size, seq_len, model_dimension)\n",
        "\n",
        "        output = self.dense(attention)\n",
        "        return output\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
        "        return output, attention_weights\n",
        "def causal_attention_mask(batch_size, n_dest, n_src):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, tf.bool)\n",
        "    mask = tf.reshape() ### shape to add batch dimension\n",
        "    return tf.tile(mask, [batch_size, 1, 1])\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        # Give code for an attention layer, feedforward layers, and normalization layers. The attention layer is first, then normalization and dropout, then forward the data passed through a non-linear function, and call the dropout layer again\n",
        "        ##\n",
        "        self.attention = MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.feed_forward = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim)\n",
        "        ])\n",
        "        ##\n",
        "        self.normalization_layer_1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.normalization_layer_2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        block_size = input_shape[1]\n",
        "        ''' Insert the calling code here '''\n",
        "        attention_output = self.attention(inputs)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "\n",
        "        out1 = self.normalization_layer_1(inputs + attention_output)\n",
        "\n",
        "        ff_output = self.feed_forward(out1)\n",
        "        ff_output = self.dropout2(ff_output)\n",
        "\n",
        "        out2 = self.normalization_layer_2(out1 + ff_output)\n",
        "        return\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_embedding = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_embedding(positions)\n",
        "        tokens = self.token_embedding(x)\n",
        "        return tokens + positions\n",
        "class Transformer(Model):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks):\n",
        "        super().__init__()\n",
        "        self.inputs = Input(shape=(maxlen,), dtype=tf.int32)\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, feed_forward_dim) for _ in range(num_transformer_blocks)]\n",
        "        self.dense = Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for i in range(self.num_transformer_blocks):\n",
        "            x = self.transformer_blocks[i](x)\n",
        "        output = self.dense(x)\n",
        "        return output\n",
        "'''Above, we have a subclass-based representation of the model, and below, a functional API-based representation\n",
        "The functional API learns much faster and more efficiently, because apparently tensorflow has a bunch of optimizations\n",
        "for static graphs which are known to it before observing the data (https://www.tensorflow.org/guide/function, functional APIs make use of this paradigm by default)\n",
        "Secondly, the for loop in the call() function cannot be optimized in the Subclass API, but it is replaced by nodes in a graph in the functional API call,\n",
        "This avoids having to shuttle between executing the fast code the people behind tensorflow have developed and a slower python for loop.\n",
        "'''\n",
        "\n",
        "def get_transformer_model(\n",
        "    maxlen,\n",
        "    vocab_size,\n",
        "    embed_dim,\n",
        "    num_heads,\n",
        "    feed_forward_dim,\n",
        "    num_transformer_blocks=1\n",
        "):\n",
        "    inputs = Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "        x = transformer_block(x)\n",
        "    outputs = Dense(vocab_size)(x)\n",
        "    model = Model(inputs=inputs, outputs=[outputs]) # This is a functional API-based representation of a tf model\n",
        "    return model\n",
        "model = get_transformer_model(block_size, input_vocab_size, feed_forward_dim, num_heads, feed_forward_dim, num_transformer_blocks)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    \"adam\",\n",
        "    loss=[loss_fn],\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "2024-12-29 20:43:08.870783: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
        "inputs = [train_data[i:i+block_size] for i in range(0, len(train_data)-block_size-1)]\n",
        "targets = [train_data[i+1:i+block_size+1] for i in range(0, len(train_data)-block_size-1)]\n",
        "\n",
        "'''\n",
        "Insert code here to preprocess the input data and the target data to send it to the model.\n",
        "'''\n",
        "\n",
        "dataset= tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "dataset = dataset.shuffle(10000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "2024-12-29 20:43:31.089570: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1027814400 exceeds 10% of free system memory.\n",
        "2024-12-29 20:43:32.144429: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1027814400 exceeds 10% of free system memory.\n",
        "model.summary()\n",
        "Model: \"functional_4\"\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "│ input_layer (InputLayer)        │ (None, 128)            │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ token_and_position_embedding    │ (None, 128, 256)       │        49,408 │\n",
        "│ (TokenAndPositionEmbedding)     │                        │               │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ transformer_block               │ (None, 128, 256)       │     2,236,160 │\n",
        "│ (TransformerBlock)              │                        │               │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ transformer_block_1             │ (None, 128, 256)       │     2,236,160 │\n",
        "│ (TransformerBlock)              │                        │               │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ transformer_block_2             │ (None, 128, 256)       │     2,236,160 │\n",
        "│ (TransformerBlock)              │                        │               │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ transformer_block_3             │ (None, 128, 256)       │     2,236,160 │\n",
        "│ (TransformerBlock)              │                        │               │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_8 (Dense)                 │ (None, 128, 65)        │        16,705 │\n",
        "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        " Total params: 9,010,753 (34.37 MB)\n",
        " Trainable params: 9,010,753 (34.37 MB)\n",
        " Non-trainable params: 0 (0.00 B)\n",
        "'''\n",
        "It will take a long time for the entirety of this function to run. However, you can always stop execution after short durations to evaluate how the code is performing. The result in the bottommost cell is after training on only 816 out of a potential 313660 batches.\n",
        "The dataset has to be shuffled between each time you call this cell to avoid running the model only on the first few input-target pairs multiple times, which may cause you to think the model is performing better than it actually is.\n",
        "If the loss is consistently than 1 even at the start, or accuracy is very high at the start, be wary. You might want to shuffle the dataset and execute again\n",
        "'''\n",
        "dataset= tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "dataset=dataset.shuffle(1000)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "model.fit(dataset, epochs=10)\n",
        "Epoch 1/10\n",
        "   55/31366 ━━━━━━━━━━━━━━━━━━━━ 23:14:12 3s/step - dense_8_accuracy: 0.5524 - loss: 1.5630\n",
        "def generate_text(model, start_index, num_generate=1):\n",
        "    '''\n",
        "        This function will generate text for num_generate characters, starting from start_index+batch_size.\n",
        "    '''\n",
        "    input_sequence = train_data[start_index:start_index + block_size]\n",
        "    generated_text = decode(input_sequence)\n",
        "    exact_sequence = decode(input_sequence)\n",
        "    for i in range(num_generate):\n",
        "        input_eval = tf.convert_to_tensor([input_sequence], dtype=tf.int32)\n",
        "        predictions, _ = model.predict(input_eval)\n",
        "        probabilities = tf.nn.softmax(predictions[0, -1]).numpy()\n",
        "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
        "        input_sequence += [next_token]\n",
        "        input_sequence = input_sequence[1:]\n",
        "        exact_sequence += decode([np.argmax(probabilities)])\n",
        "        generated_text += decode([next_token])\n",
        "\n",
        "    return generated_text, exact_sequence\n",
        "generate_text(model, start_index=0, num_generate=1000)\n"
      ]
    }
  ]
}